{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Breast Cancer Classification by Using Iterative Soft Thresholding for LASSO**\n",
        "\n",
        "Course: Math 535 - Mathematical Methods in Data Science (MMiDS) - Spring 2024\n",
        "\n",
        "Name: Junyi Liu; Weibing Wang"
      ],
      "metadata": {
        "id": "alSVqprOCXCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1 - Background and motivation**"
      ],
      "metadata": {
        "id": "3k2H5nGEpBmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Breast cancer remains one of the most common and devastating forms of cancer worldwide, affecting millions of women each year.\n",
        "\n",
        "The advent of genomic technologies has revolutionized cancer research, enabling the analysis of vast arrays of genetic data to uncover biomarkers and genetic mutations associated with breast cancer. However, the high dimensionality of genomic data presents significant challenges. Typical datasets may contain expressions of tens of thousands of genes, yet only a small fraction of these genes are relevant for diagnosing or understanding the pathology of breast cancer. Diagnosing breast cancer using traditional least squares methods can easily lead to overfitting problems. This scenario necessitates robust feature selection methods to identify the most informative genetic features for binary classification.\n",
        "\n",
        "Linear models with L1 regularization, commonly known as LASSO (Least Absolute Shrinkage and Selection Operator), have emerged as a powerful tool for feature selection in high-dimensional datasets. However, the computational complexity of applying LASSO in very high-dimensional spaces and the challenge of tuning the regularization parameter, which dictates the sparsity of the solution, can be prohibitive. Therefore we use ISTA (Iterative Shrinkage-Thresholding Algorithm) which applies the gradient descent and soft thresholding operator to solve the LASSO regression problem.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jkmgmJO0CIAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 - Theory**"
      ],
      "metadata": {
        "id": "oY3HsM3no9hN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "**2.1 LASSO** (Least Absolute Shrinkage and Selection Operator)\n",
        "\n",
        "\n",
        "Before discussing the ISTA, let us first clarify the objective function of LASSO, as ISTA is primarily designed to optimize this objective function.\n",
        "\n",
        "The objective function for LASSO regression is given by:\n",
        "\\begin{equation}\n",
        "    \\min_{\\beta} \\left( \\frac{1}{2n} \\|y - X\\beta\\|^2_2 + \\lambda \\|\\beta\\|_1 \\right)\n",
        "\\end{equation}\n",
        "where:\n",
        "- $y$ is the response variable vector.\n",
        "- $X$ is the design matrix containing features of observations.\n",
        "- $\\beta$ is the coefficient vector that needs to be solved.\n",
        "- $\\|y - X\\beta\\|_2^2$, represents the sum of squared residuals, indicating the model error, similar to the least squares.\n",
        "- $\\|\\lambda \\beta\\|_1$ is the L1 regularization term controlled by the regularization parameter $\\lambda$  , which is used to penalize the sum of the absolute values of the coefficients to control the complexity of the model, some coefficient values need to be reduced to zero. Such treatment aids in feature selection, automatically identifying the features that contribute most to the model while suppressing those that are irrelevant to avoid overfitting.\n",
        "- $\\lambda$ is the regularization parameter that controls the strength of the L1 regularization term.\n",
        "- $n$ is the number of samples.\n",
        "\n",
        "\n",
        "---\n",
        "Consider a convex objective of the form\n",
        "\n",
        "\\begin{equation}\n",
        "f(\\theta) = L(\\theta) + R(\\theta)\n",
        "\\end{equation}\n",
        "\n",
        "Here $L(\\theta)$ is a convex and differentiable loss function, while $R(\\theta)$ is a convex but possibly non-differentiable regularization term. When using ISTA, we typically use the unconstrained form of the LASSO problem where $L(\\theta)$ is a squared loss term and $R(\\theta) $is the L1 norm $\\lambda \\|\\beta\\|_1$. Due to the presence of the L1 term, directly applying standard gradient descent is not applicable because the L1 norm is not differentiable at zero. This introduces the concept of the proximal operator.\n"
      ],
      "metadata": {
        "id": "PEFfmbGYuWQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Proximal Operator**\n",
        "\n",
        "Proximal Operator is a specific mathematical tool used to solve optimization problems that contain non-differentiable function parts $R(\\theta)$.\n",
        "\n",
        "For a regularization term $R(\\theta)$and a parameter $v$, the proximal operator is defined as:\n",
        "\\begin{equation}\n",
        "\\text{prox}_{R}(v) = \\arg\\min_{\\theta} \\left\\{ R(\\theta) + \\frac{1}{2} \\|\\theta - v\\|_2^2 \\right\\}\n",
        "\\end{equation}\n",
        "Here, the goal is to find a point $\\theta$ that minimizes the sum of the regularization term and the squared Euclidean distance between $\\theta$ and $v$.\n",
        "\n"
      ],
      "metadata": {
        "id": "49jJrlWnCH9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3 Soft Thresholding Operator**\n",
        "\n",
        "For L1 regularization, the proximal operator is specifically expressed through the soft thresholding operator, which is given by:\n",
        "\n",
        "\\begin{equation}\n",
        "S_{\\lambda}(\\beta_i) = \\text{sign}(\\beta_i) \\max(|\\beta_i| - \\lambda, 0)\n",
        "\\end{equation}\n",
        "\n",
        "here $S_{\\lambda}(\\beta_i)$ represents the soft thresholding operation applied to each coefficient $\\beta_i$, sign($\\beta_i$) represents the Sign function:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{sign}(x) =\n",
        "\\begin{cases}\n",
        "-1 & \\text{if } x < 0 \\\\\n",
        "0 & \\text{if } x = 0 \\\\\n",
        "1 & \\text{if } x > 0\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Derivation of Soft Thresholding Operator:\n",
        "Considering the L1 regularization term $R(\\theta) = \\lambda\\|\\theta\\|_1$, we want to compute its proximal operator. We face the following optimization problem:\n",
        "\n",
        "\\begin{equation}\n",
        "\\arg\\min_{\\theta} \\left\\{ \\lambda\\|\\theta\\|_1 + \\frac{1}{2} \\|\\theta - v\\|_2^2 \\right\\}\n",
        "\\end{equation}\n",
        "\n",
        "Due to the additivity, for a single coefficient $\\theta_i\\$, the problem simplifies to:\n",
        "\n",
        "\\begin{equation}\n",
        "\\arg\\min_{\\theta_i} \\left\\{ \\lambda|\\theta_i| + \\frac{1}{2} (\\theta_i - v_i)^2 \\right\\}\n",
        "\\end{equation}\n",
        "\n",
        "To minimize this function, we can consider the positive and negative values of $\\theta_i$ separately. For $\\theta_i > 0$, the the function becomes:\n",
        "\n",
        "\\begin{equation}\n",
        "\\arg\\min_{\\theta_i} \\left\\{ \\lambda\\theta_i + \\frac{1}{2} (\\theta_i - v_i)^2 \\right\\}\n",
        "\\end{equation}\n",
        "\n",
        "We can minimize this function by setting its derivative to zero:\n",
        "\n",
        "\\begin{equation}\n",
        "\\lambda +  (\\theta_i - v_i) = 0\n",
        "\\end{equation}\n",
        "It is easy to notice that the second derivative is 1.\n",
        "\n",
        "Solving gives:\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_i = v_i - \\lambda\n",
        "\\end{equation}\n",
        "\n",
        "Similarly, for the case $\\theta_i < 0$, we get:\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_i = v_i + \\lambda\n",
        "\\end{equation}\n",
        "\n",
        "Combining these two cases, and considering that when $|v_i| \\leq \\lambda$. The absolute value function $|\\theta_i|$ is not differentiable at 0, which means that the minimum point cannot be found directly by setting the derivative to 0, the optimal solution is $\\theta_i = 0$, because setting $\\theta_i$ to zero minimizes the function in this case, we arrive at the definition of the soft thresholding operator:\n",
        "\n",
        "\\begin{equation}\n",
        "S_{\\lambda}(\\beta_i) = \\text{sign}(\\beta_i) \\max(|\\beta_i| - \\lambda, 0)\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "--JPbp4FCaLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4 Iterative Shrinkage-Thresholding Algorithm (ISTA)**\n",
        "\n",
        "The objective function for LASSO regression is given by:\n",
        "\\begin{equation}\n",
        "    \\min_{\\beta} \\left( \\frac{1}{2n} \\|y - X\\beta\\|^2_2 + \\lambda \\|\\beta\\|_1 \\right)\n",
        "\\end{equation}\n",
        "\n",
        "The first term is the squared error term, and the second term is the L1 regularization term.\n",
        "\n",
        "Each step of the ISTA can be divided into two main parts:\n",
        "\n",
        "**Part 1: Gradient descent**\n",
        "\n",
        "At each step, we perform gradient descent on the loss function $L(\\beta)$ to update the coefficients. The update rule is given by:\n",
        "\n",
        "\\begin{equation}\n",
        "\\beta^{(k+1)} = \\beta^{(k)} - \\alpha \\nabla L(\\beta^{(k)})\n",
        "\\end{equation}\n",
        "Here, $\\alpha$ is the step size, and $\\beta^{(k)}$ denotes the coefficient vector at the $k$-th iteration.\n",
        "\n",
        "**Part 2: Soft Thresholding**\n",
        "\n",
        "After updating the coefficients using gradient descent to obtain $\\beta^{(k+1)}$, we apply the soft thresholding operation to each coefficient, yielding:\n",
        "\n",
        "\\begin{equation}\n",
        "\\beta_j^{(k+1)} = \\text{sign}(\\beta_j^{(k+1)}) \\max(|\\beta_j^{(k+1)}| - \\lambda, 0)\n",
        "\\end{equation}\n",
        "\n",
        "Here, $\\lambda$ is the regularization parameter of the LASSO method, $\\text{sign}(\\beta_j)$ is the sign function.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The specific steps of the ISTA are as follows:\n",
        "\n",
        "1. Initialize $\\beta^{(0)}$ (usually with zeros).\n",
        "2. For each iteration:\n",
        "\n",
        "\n",
        "  *   Compute the gradient of the loss function $L(\\beta^{(k)})$.\n",
        "  *   Update the coefficients with gradient descent:\n",
        "  \\begin{equation}\n",
        "  \\beta^{(k+1)} = \\beta^{(k)} - \\alpha \\nabla L(\\beta^{(k)})\n",
        "  \\end{equation}\n",
        "\n",
        "  *   Apply the soft-thresholding operator to each coefficient:\n",
        "  \\begin{equation}\n",
        "  \\beta_j^{(k+1)} \\leftarrow \\text{sign}(\\beta_j^{(k+1)}) \\max\\left(|\\beta_j^{(k+1)}| - \\lambda, 0\\right)\n",
        "  \\end{equation}\n",
        "\n",
        "3. Convergence check: Terminate if the difference between successive iterations is below a threshold or if the maximum number of iterations has been reached.\n"
      ],
      "metadata": {
        "id": "xZgR65BuTYgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3 - Implementation:**\n"
      ],
      "metadata": {
        "id": "CvI5tGM-nj1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "RKfd69UUXtRd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv('Breast_classes2.csv')\n",
        "\n",
        "print(\"datashape：\", df.shape)\n",
        "\n",
        "print(\"datatype：\", df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqPV0haiXu-A",
        "outputId": "8da89908-97ee-41bd-e001-7fa0c033c252"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datashape： (289, 35983)\n",
            "datatype： samples                            object\n",
            "type                               object\n",
            "NM_144987                         float64\n",
            "NM_013290                         float64\n",
            "ENST00000322831                   float64\n",
            "                                   ...   \n",
            "lincRNA:chr9:4869500-4896050_F    float64\n",
            "NM_016053                         float64\n",
            "NM_001080425                      float64\n",
            "ENST00000555638                   float64\n",
            "ENST00000508993                   float64\n",
            "Length: 35983, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq8jpAiYY9KD",
        "outputId": "3031af45-9927-4381-fc72-16bd144e1758"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         samples    type  NM_144987  \\\n",
            "0  GSM1823702_252800417016_S01_GE1_107_Sep09_1_2  normal   8.693318   \n",
            "1  GSM1823703_252800417016_S01_GE1_107_Sep09_2_1  normal   9.375980   \n",
            "2  GSM1823704_252800416877_S01_GE1_107_Sep09_2_3  normal   8.943442   \n",
            "3  GSM1823705_252800416894_S01_GE1_107_Sep09_1_1  normal   9.020798   \n",
            "4  GSM1823706_252800416894_S01_GE1_107_Sep09_1_3  normal   8.806154   \n",
            "\n",
            "   NM_013290  ENST00000322831  NM_001625  lincRNA:chr7:226042-232442_R  \\\n",
            "0   7.718016         6.044438  10.747077                      9.133777   \n",
            "1   7.072232         6.976741  10.429671                      9.526500   \n",
            "2   7.964573         6.269055  10.825025                      9.396855   \n",
            "3   7.824639         6.165165  11.646788                      8.776462   \n",
            "4   7.555348         6.230969  11.635247                      8.911383   \n",
            "\n",
            "   NM_032391  ENST00000238571  XR_108906  ...  \\\n",
            "0   4.735581         5.634732   4.670231  ...   \n",
            "1   5.221089         5.425187   4.860931  ...   \n",
            "2   5.258506         5.824921   4.964604  ...   \n",
            "3   4.648655         6.676692   4.770186  ...   \n",
            "4   4.518054         6.520691   4.540453  ...   \n",
            "\n",
            "   lincRNA:chr4:77860976-77869926_F  NM_152343  NM_001005327  NM_001039355  \\\n",
            "0                          7.570363   6.368684      4.784042     10.747723   \n",
            "1                          7.903335   5.713115      4.421074     11.299200   \n",
            "2                          7.705765   6.595364      4.410870     10.576807   \n",
            "3                          6.633058   5.786781      4.572984     11.175090   \n",
            "4                          6.211581   5.538635      4.613828     12.014365   \n",
            "\n",
            "   lincRNA:chr21:44456656-44468556_R  lincRNA:chr9:4869500-4896050_F  \\\n",
            "0                           5.090500                        5.994149   \n",
            "1                           4.447052                        4.421074   \n",
            "2                           5.003699                        6.529257   \n",
            "3                           4.990888                        6.669871   \n",
            "4                           4.979883                        6.414621   \n",
            "\n",
            "   NM_016053  NM_001080425  ENST00000555638  ENST00000508993  \n",
            "0  10.649336      8.969439         4.985693         5.090500  \n",
            "1  10.746854      8.174489         4.464177         4.536891  \n",
            "2  10.430034      8.473468         4.668447         5.084127  \n",
            "3  11.110395      8.880818         4.537626         4.648655  \n",
            "4  10.909805      9.526500         4.670490         4.613828  \n",
            "\n",
            "[5 rows x 35983 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "type_mapping = {\n",
        "    'normal': 0,\n",
        "    'breast_adenocarcinoma': 1\n",
        "}\n",
        "\n",
        "df['type_encoded'] = df['type'].map(type_mapping)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTT_4wBxYgFI",
        "outputId": "d898b3f1-3af0-4270-93c5-9987130db828"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         samples    type  NM_144987  \\\n",
            "0  GSM1823702_252800417016_S01_GE1_107_Sep09_1_2  normal   8.693318   \n",
            "1  GSM1823703_252800417016_S01_GE1_107_Sep09_2_1  normal   9.375980   \n",
            "2  GSM1823704_252800416877_S01_GE1_107_Sep09_2_3  normal   8.943442   \n",
            "3  GSM1823705_252800416894_S01_GE1_107_Sep09_1_1  normal   9.020798   \n",
            "4  GSM1823706_252800416894_S01_GE1_107_Sep09_1_3  normal   8.806154   \n",
            "\n",
            "   NM_013290  ENST00000322831  NM_001625  lincRNA:chr7:226042-232442_R  \\\n",
            "0   7.718016         6.044438  10.747077                      9.133777   \n",
            "1   7.072232         6.976741  10.429671                      9.526500   \n",
            "2   7.964573         6.269055  10.825025                      9.396855   \n",
            "3   7.824639         6.165165  11.646788                      8.776462   \n",
            "4   7.555348         6.230969  11.635247                      8.911383   \n",
            "\n",
            "   NM_032391  ENST00000238571  XR_108906  ...  NM_152343  NM_001005327  \\\n",
            "0   4.735581         5.634732   4.670231  ...   6.368684      4.784042   \n",
            "1   5.221089         5.425187   4.860931  ...   5.713115      4.421074   \n",
            "2   5.258506         5.824921   4.964604  ...   6.595364      4.410870   \n",
            "3   4.648655         6.676692   4.770186  ...   5.786781      4.572984   \n",
            "4   4.518054         6.520691   4.540453  ...   5.538635      4.613828   \n",
            "\n",
            "   NM_001039355  lincRNA:chr21:44456656-44468556_R  \\\n",
            "0     10.747723                           5.090500   \n",
            "1     11.299200                           4.447052   \n",
            "2     10.576807                           5.003699   \n",
            "3     11.175090                           4.990888   \n",
            "4     12.014365                           4.979883   \n",
            "\n",
            "   lincRNA:chr9:4869500-4896050_F  NM_016053  NM_001080425  ENST00000555638  \\\n",
            "0                        5.994149  10.649336      8.969439         4.985693   \n",
            "1                        4.421074  10.746854      8.174489         4.464177   \n",
            "2                        6.529257  10.430034      8.473468         4.668447   \n",
            "3                        6.669871  11.110395      8.880818         4.537626   \n",
            "4                        6.414621  10.909805      9.526500         4.670490   \n",
            "\n",
            "   ENST00000508993  type_encoded  \n",
            "0         5.090500             0  \n",
            "1         4.536891             0  \n",
            "2         5.084127             0  \n",
            "3         4.648655             0  \n",
            "4         4.613828             0  \n",
            "\n",
            "[5 rows x 35984 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, 2:-1].values  # 假设特征在第三列到倒数第二列\n",
        "y = df['type_encoded'].values\n",
        "# 标准化特征（可选，但推荐）\n",
        "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
        "# 添加截距项\n",
        "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
        "m, n = X.shape\n",
        "print(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l68oz9iya6cH",
        "outputId": "a2f2fb72-1dba-4799-d536-f7f00d134989"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.         -0.29044738  0.15716683 ...  0.29101254  0.57661691\n",
            "   1.16698088]\n",
            " [ 1.          1.44237276 -0.96712986 ... -1.01273948 -0.75420077\n",
            "  -0.79655215]\n",
            " [ 1.          0.34444946  0.58641694 ... -0.52240203 -0.23294027\n",
            "   1.1443803 ]\n",
            " ...\n",
            " [ 1.          0.61844411  0.04204796 ... -0.05398532 -0.51800249\n",
            "  -0.47654098]\n",
            " [ 1.         -1.75940706  0.81641611 ...  1.42053716 -0.62246924\n",
            "  -0.63175092]\n",
            " [ 1.          0.05034099 -0.15467297 ...  1.55371374  0.49169946\n",
            "   0.03375616]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def soft_thresholding(x, lambda_):\n",
        "    return np.sign(x) * np.maximum(np.abs(x) - lambda_, 0)\n",
        "\n",
        "def ista(X, y, lambda_, alpha, n_iters, tol=1e-4, patience=20):\n",
        "    m, n = X.shape\n",
        "    beta = np.zeros(n)\n",
        "    losses = []\n",
        "    best_loss = float('inf')\n",
        "    not_improved_count = 0\n",
        "\n",
        "    for i in range(n_iters):\n",
        "        prediction = X.dot(beta)\n",
        "        error = prediction - y\n",
        "        current_loss = (1 / (2 * m)) * np.dot(error, error) + lambda_ * np.sum(np.abs(beta))\n",
        "        losses.append(current_loss)\n",
        "\n",
        "        # Check for early stopping\n",
        "        if current_loss < best_loss - tol:\n",
        "            best_loss = current_loss\n",
        "            not_improved_count = 0\n",
        "        else:\n",
        "            not_improved_count += 1\n",
        "            if not_improved_count > patience:\n",
        "                print(f\"Early stopping after {i + 1} iterations\")\n",
        "                break\n",
        "\n",
        "        gradient = X.T.dot(error) / m\n",
        "        beta = beta - alpha * gradient\n",
        "        beta = soft_thresholding(beta, lambda_ * alpha)\n",
        "\n",
        "    return beta, losses"
      ],
      "metadata": {
        "id": "XQDB_t-wbfZW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import itertools\n",
        "\n",
        "def cross_validate_ista_accuracy(X, y, lambdas, alphas, k, n_iters):\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    best_params = {}\n",
        "    best_score = 0  # 初始化为0，因为我们希望找到最高的准确率\n",
        "\n",
        "    # 迭代所有lambda和alpha的组合\n",
        "    for lambda_, alpha in itertools.product(lambdas, alphas):\n",
        "        scores = []\n",
        "\n",
        "        for train_index, test_index in kf.split(X):\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "            beta, _ = ista(X_train, y_train, lambda_, alpha, n_iters)\n",
        "\n",
        "            # 计算验证集上的准确率\n",
        "            predictions = X_test.dot(beta)\n",
        "            predicted_labels = (predictions > 0.5).astype(int)\n",
        "            accuracy = accuracy_score(y_test, predicted_labels)\n",
        "            scores.append(accuracy)\n",
        "\n",
        "        mean_score = np.mean(scores)\n",
        "        if mean_score > best_score:\n",
        "            best_score = mean_score\n",
        "            best_params = {'lambda': lambda_, 'alpha': alpha}\n",
        "\n",
        "    return best_params, best_score\n",
        "\n"
      ],
      "metadata": {
        "id": "PO3m-de7ep2I"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 参数范围\n",
        "lambdas = np.logspace(-4, 1, 6)  # 从1e-4到1e1\n",
        "alphas = np.logspace(-5, -2, 4)  # 从1e-5到1e-2\n",
        "\n",
        "# 执行交叉验证\n",
        "best_params, best_score = cross_validate_ista_accuracy(X, y, lambdas, alphas, k=5, n_iters=1000)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Validation Score:\", best_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHgaeL39e1u1",
        "outputId": "f448ec46-c383-40c6-f609-dd9848c7f6d9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 881 iterations\n",
            "Early stopping after 884 iterations\n",
            "Early stopping after 857 iterations\n",
            "Early stopping after 993 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Early stopping after 22 iterations\n",
            "Best Parameters: {'lambda': 0.0001, 'alpha': 0.001}\n",
            "Best Validation Score: 0.778584392014519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for fn in uploaded.keys():\n",
        "#    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#      name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "id": "cmexOC1S1C5C"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4 - Data:**"
      ],
      "metadata": {
        "id": "es2tyqr9oNOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1 Provenance of the datasets**\n",
        "\n",
        "We chose a dataset for breast cancer classification from the Curated Microarray Database, a repository containing 78 handpicked cancer microarray datasets, extensively curated from 30.000 studies from the Gene Expression Omnibus (GEO), solely for machine learning.\n",
        "\n",
        "**4.2 Goal of the analysis**\n",
        "\n",
        "Our goal is that after inputting the characteristic genes, our model can accurately diagnose whether the patient has breast cancer, and the model will output normal or breast cancer.\n",
        "\n",
        "**4.3 Dataset introduction**\n",
        "\n",
        "Our dataset contains 289 samples with 35,982 characterized genes and has two diagnostic types, normal and breast_adenocarcinoma. In this dataset, the number of samples in it is much less than the number of features, which is a classic LASSO problem, and also the number of normal and breast_adenocarcinoma is very well balanced as 146:143, so the dataset is effective in avoiding overfitting and false negative problems. Curated Microarray Database has provided the prediction accuracy of many machine learning models for our chosen dataset, 0.93 for SVM, 0.8 for Decision Tree, 0.7 for Multilayer Perceptron, 0.82 for KNN, and 0.78 for K-MEANS, which were able to be compared to our ISTA.\n"
      ],
      "metadata": {
        "id": "dBahf8iIoSOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5 - References:**\n",
        "\n",
        "Feltes, B.C.; Chandelier, E.B.; Grisci, B.I.; Dorn, M. CuMiDa: An Extensively Curated Microarray Database for Benchmarking and Testing of Machine Learning Approaches in Cancer Research. Journal of Computational Biology, 2019.\n",
        "\n",
        "Section 13.4.3 in K. P. Murphy, Machine Learning: A Probabilistic Perspective, MIT, 2012\n",
        "\n",
        "Section 10.6 in Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2, 3rd Edition 3rd ed. Edition\n",
        "by Sebastian Raschka, Vahid Mirjalili\n"
      ],
      "metadata": {
        "id": "DtZTwhxm4iao"
      }
    }
  ]
}